\section{Related Work}
\label{section:related_work2}

Although there has been no research on similarity inclusion dependencies yet, three research areas overlap with this work: traditional inclusion dependency discovery, other relaxations of dependencies, and string similarity joins.

Inclusion dependencies~(INDs) are a well-known type of data dependency and have several algorithms to discover them from data--- see~\citeauthor{dursch2019eval} for an overview of approaches~\cite{dursch2019eval}.
There are approaches for both unary and n-ary IND discovery.
\citeauthor{deMarchiIND} presented an early algorithm to discover unary INDs by intersecting the attribute sets for each value~\cite{deMarchiIND}.
\algorithmName{Many} also discovers unary INDs, but focuses on the problem of finding INDs in millions of tables~\cite{Tschirschnitz2017MANY}.
A prominent representative for n-ary, i.e., inclusion between tuples of attribute lists, IND discovery is \algorithmName{Zigzag}~\cite{deMarchi2003ZIGZAG}.
It combines both up- and downwards pruning to discover maximal n-ary INDs efficiently.


In particular, \sawfish adapts a few techniques from the algorithm \algorithmName{Binder}~\cite{papenbrock2015divide}.
Initially, \algorithmName{Binder} splits the input data into buckets for each attribute. 
It uses hash-partitioning to distribute the values evenly and place equal values into the same bucket.
If main memory is exhausted, the largest buckets are written to disk, allowing \algorithmName{Binder} to scale to large datasets.
To validate IND candidates, \algorithmName{Binder} loads data partitions into the main memory.
Each partition contains one bucket from each attribute with the same bucket number.
As the bucket number relates to the hash value, equal values of different attributes are in the same partition.
If a partition is too large for main memory, it can be lazily refined into subpartitions.
\algorithmName{Binder} deduces which INDs still hold in the data for each partition and prunes the other candidates.
After processing every partition, \algorithmName{Binder} outputs all remaining candidates as valid INDs.

The input-handling of \sawfish also splits the column values into different buckets.
However, it cannot use hash-partitioning due to the similarity measure, so it uses an adaptive main memory handling---presented in detail in Section~\ref{section:sawfish:preprocessing}.
\sawfish also loads chunks of data into main memory and checks remaining sIND candidates to validate sIND candidates.
We detail our validation strategy in Section~\ref{section:impl:sawfish}.

Similarly to how sINDs extend the concept of INDs, there are examples of such relaxations of other dependencies--- see~\citeauthor{caruccio2016relaxed} for an overview of relaxed FD approaches~\cite{caruccio2016relaxed}.
As mentioned in the introduction, matching dependencies (MDs) are a prominent extension of functional dependencies (FDs).
Like sINDs, they incorporate a similarity measure to find additional dependencies in the data.
MDs were first introduced by \textcite{fan2008dependencies}.
\citeauthor{MDDiscovery} showed how to efficiently find all MDs in a database~\cite{MDDiscovery}.
Their \algorithmName{HyMD} algorithm combines two techniques to find MDs: lattice traversal and inference from record pairs.
The lattice comprises all candidate MDs in a sorted order.
Therefore, it can be traversed to find minimal MDs.
To quickly find counterexamples for an MD candidate, \algorithmName{HyMD} can compare record pairs and infer which MDs might still be minimal.
After comparing every record pair, the inferred MDs are the correct solution set.
We cannot use inference from record pairs, because we cannot infer the validity of an sIND from only a record pair.
Since a single value can be similar to multiple other values, a record pair that shows that a value is not similar to another value does not disprove the validity of an sIND.
\algorithmName{HyMD} computes the similarity of every value pair beforehand to access it quickly when validating MDs.
Our approach avoids this preprocessing and computes the similarity while validating.
Thus, we can save many unnecessary computations, because only those value pairs that we actually process are compared.

Finally, string similarity joins and sINDs share a related sub-problem.
Like sINDs, for a large set of strings, string similarity joins need to identify similar strings based on some similarity measure to execute the join.
\citeauthor{StringSimSurvey} provide an overview of different approaches~\cite{StringSimSurvey}.
Most methods either use specific substrings or a tree-like data structure to compute the similarity of two strings.
For example, the algorithm \algorithmName{TrieJoin} uses a trie to efficiently calculate the similarity~\cite{feng2012trie}.


\sawfish uses the underlying method of \algorithmName{PassJoin}~\cite{PassJoin} to find similar strings for a dependent value when using the edit distance as its similarity measure.
The author's observation is based on the pigeonhole principle: Assume an edit distance threshold $\tau$, two strings $x,y$ and $ED(x,y) \leq \tau$. 
If we split $x$ into $\tau + 1$ disjoint segments, there exists a substring of $y$ that is equal to one of the segments.
Otherwise, we would need at least one edit operation for each segment to transform it to a substring of $y$.
However, this violates our assumption that $ED(x,y) \leq \tau$.
For example, given the two soccer club names, \emph{Potsdamer SC} and \emph{SpVgg Beelitz}, we want to know if their edit distance is within one, i.e. $\tau = 1$.
Therefore, we segment Potsdamer SC in $\tau + 1 = 2$ equally-sized segments: \data{Potsda} and \data{mer SC}.
We observe that there is no substring in \data{SpVgg Beelitz} that matches any of the segments.
Therefore, we know that these club names are not similar without computing their actual edit distance.
This segment filter effectively prunes dissimilar string pairs.
Moreover, based on this filter, an inverted index of the segments to the indexed elements can be built.
Additionally, \citeauthor{PassJoin} presented techniques to reduce the number of substrings that need to be compared to the segments.
They also improve the exact edit distance computation based on their segmentation filter.
We detail our usage of the validation techniques in Section~\ref{section:sind}.

When using the Jaccard similarity, \sawfish uses and adapts the \algorithmName{ScanCount}~\cite{StringSimSurvey} method.
Assume a given Jaccard similarity threshold~$\delta$, a string~$x$ and a set of strings~$Y$.
First, for each $y \in Y$, it stores a mapping of each token to its parent string.
For each token of $x$, \algorithmName{ScanCount} retrieves the list of parent strings that contain that token and maintains a count of each occurrence of each parent string in all lists.
Next, it computes a threshold $T = \frac{\delta}{1+\delta}(|\text{token}(x)| + |\text{min(token}(y))|)$.
Afterwards, all strings $y \in Y$ that have a count $\geq T$ are directly compared to $x$ to compute their actual Jaccard similarity.
We improve this version for our needs and show the modified version in Section~\ref{section:sind}.
Note that we cannot use the popular \algorithmName{minHash}~\cite{Wu2022MinHash} to discover similar strings, because it is only an estimation of the Jaccard similarity.
